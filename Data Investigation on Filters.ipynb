{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "f = open(\"data\", \"r\")\n",
    "data_str = f.read()\n",
    "data = ast.literal_eval(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# familiarize with keys if needed \n",
    "# key_set = []\n",
    "\n",
    "# for dictionary in data:\n",
    "#     for key in dictionary:\n",
    "#         key_set.append(key)\n",
    "    \n",
    "# key_set = set(key_set)\n",
    "# print key_set\n",
    "\n",
    "\n",
    "\n",
    "# 'link' and 'caption' has info on the link\n",
    "# to do: connect with users to find how many places they end up visiting just from fb\n",
    "\n",
    "# explore key if needed\n",
    "# type_split = {}\n",
    "# key = 'link'\n",
    "\n",
    "# for dictionary in data:\n",
    "#     if key in dictionary:\n",
    "#         if dictionary['type'] in type_split:\n",
    "#             type_split[dictionary['type']].append(dictionary[key])\n",
    "#         else:\n",
    "#             type_split[dictionary['type']] = [dictionary[key]]\n",
    "            \n",
    "# for x in type_split:\n",
    "#     print x\n",
    "#     print \"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "#     for y in type_split[x]:\n",
    "#         print y\n",
    "#         print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = {}\n",
    "        \n",
    "for dictionary in data:\n",
    "    # omit events since they are not reliable sources of information\n",
    "    if dictionary['type'] not in ('event'):\n",
    "        \n",
    "        # user *posting*\n",
    "        # eventually group by poster to avoid duplication\n",
    "        user = dictionary['from']['id']\n",
    "        \n",
    "        # if the user is not found, add an empty list to the dictionary\n",
    "        if not user in content:\n",
    "            content[user] = []\n",
    "        \n",
    "        # add description\n",
    "        if 'description' in dictionary:\n",
    "            content[user].append(dictionary['description'])\n",
    "        \n",
    "        # add message\n",
    "        if 'message' in dictionary:\n",
    "            content[user].append(dictionary['message'])\n",
    "            \n",
    "        # add name if not in photo, since names of photos are not useful\n",
    "        if dictionary['type'] not in ('photo') and 'name' in dictionary:\n",
    "            content[user].append(dictionary['name'])\n",
    "            \n",
    "        # add any comments\n",
    "        if 'comments' in dictionary:\n",
    "            # each post has a set of comments\n",
    "            for comment in dictionary['comments']['data']:\n",
    "                # append the actual comment string\n",
    "                content[user].append(comment['message']) \n",
    "#                 print comment['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the text by word, clean it and simplify words into stems\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Porter stemmer is a pre-trained stemmer that finds the most likely stem for an english word\n",
    "from gensim.parsing import PorterStemmer\n",
    "global_stemmer = PorterStemmer()\n",
    "\n",
    "# NLTK stopwords to remove (in addition to the scikit-learn default)\n",
    "from nltk.corpus import stopwords\n",
    "myStop = stopwords.words('english')\n",
    "myStop = list(set(myStop) - set(['only', 'has']))\n",
    "# print myStop\n",
    "\n",
    "# Remove unicode, normalizing it to ascii\n",
    "import unicodedata\n",
    "\n",
    "# Custom tokenizer\n",
    "def tokenize(doc):\n",
    "    # Clean the document by normalizing to ascii\n",
    "    doc = unicode(doc, 'utf-8', errors='ignore')\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ascii','ignore')        \n",
    "\n",
    "    # Remove hyphens and dashes\n",
    "    doc = re.sub('[-â€”]', ' ', doc)\n",
    "    \n",
    "    # Strip whitespace\n",
    "    doc = doc.strip()\n",
    "\n",
    "    # First tokenize by sentence then by word\n",
    "    tokens = [word for sent in nltk.sent_tokenize(doc) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    filtered_tokens = []\n",
    "    # Filter out any tokens not containing at least 2 letters\n",
    "    for token in tokens:\n",
    "        if len(re.sub('[^a-zA-Z]', '', token)) > 2:\n",
    "            # and word not in myStop\n",
    "            # Stem the token\n",
    "            filtered_tokens.append(global_stemmer.stem(token))      \n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find specific filters like pets, male/female only, has parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'femal', 'pet', 'park', 'gender', 'intern']\n",
      "['ac', 'intern', 'girl', 'facil', 'privat', 'pet', 'internet', 'condit', 'gym', 'park', 'bathroom', 'util', 'bu', 'mainten', 'laundri', 'kitchen', 'femal', 'gender', 'gui', 'wifi', 'dog', 'cat', 'boi', 'male', 'furnish']\n"
     ]
    }
   ],
   "source": [
    "# Before doing this, get an idea of common words surrounding the below keywords (stemmed)\n",
    "main = [global_stemmer.stem(x) for x in ['male', 'female', 'pet', 'parking', 'gender', 'intern']]\n",
    "print main\n",
    "\n",
    "keywords = [global_stemmer.stem(x) for x in ['male', 'female', 'pet', 'parking', 'gender', 'intern',\n",
    "    'furnish',\n",
    "    'internet',\n",
    "    'bathroom',\n",
    "    'utilities',\n",
    "    'util',\n",
    "    'laundry',\n",
    "    'wifi',\n",
    "    'girl',\n",
    "    'boy',\n",
    "    'guy',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'facility',\n",
    "    'facilities',\n",
    "    'gym',\n",
    "    'private',\n",
    "    'ac',\n",
    "    'conditioning',\n",
    "    'maintenance',\n",
    "    'kitchen',\n",
    "    'bus'\n",
    "    ]]\n",
    "\n",
    "keywords = list(set(keywords))\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_tokenized = {}\n",
    "\n",
    "for user in content:\n",
    "    content_tokenized[user] = []\n",
    "    for info in content[user]:\n",
    "        temp = tokenize(info)\n",
    "        if len(temp) > 1:\n",
    "            content_tokenized[user].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# could be duplication somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbours(my_list,index,depth):\n",
    "    lower = max(0, index-depth)\n",
    "    upper = min(len(my_list), index+1+depth)\n",
    "    \n",
    "    before = []\n",
    "    after = []\n",
    "    \n",
    "    if lower < index:\n",
    "        before = my_list[lower:index]\n",
    "    if upper > index:\n",
    "        after = my_list[index+1:upper]\n",
    "    \n",
    "    before.extend(after)\n",
    "    return before\n",
    "\n",
    "keyword_neighbours = {}\n",
    "for keyword  in keywords:\n",
    "    keyword_neighbours[keyword] = []\n",
    "\n",
    "for user in content_tokenized:\n",
    "    for content in content_tokenized[user]:\n",
    "        for i, word in enumerate(content):\n",
    "            if word in keywords:\n",
    "                keyword_neighbours[word].append(neighbours(content, i, 2))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "ac\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "intern\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "centr: 1\n",
      "govern: 1\n",
      "for: 1\n",
      "innov: 1\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "girl\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "all: 15\n",
      "for: 7\n",
      "unit: 6\n",
      "fulli: 6\n",
      "with: 5\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "facil\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "laundri: 21\n",
      "free: 9\n",
      "park: 8\n",
      "site: 7\n",
      "picnic: 6\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "bathroom\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "with: 76\n",
      "share: 57\n",
      "bedroom: 56\n",
      "ensuit: 54\n",
      "room: 47\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "pet\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "friendli: 12\n",
      "free: 9\n",
      "and: 7\n",
      "smoke: 7\n",
      "fulli: 5\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "internet\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "speed: 192\n",
      "high: 168\n",
      "includ: 167\n",
      "util: 122\n",
      "and: 98\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "condit\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "air: 69\n",
      "includ: 23\n",
      "and: 10\n",
      "min: 9\n",
      "walk: 8\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "gym\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "room: 21\n",
      "studi: 14\n",
      "and: 12\n",
      "game: 9\n",
      "loung: 7\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "park\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "avail: 104\n",
      "laundri: 77\n",
      "site: 76\n",
      "free: 51\n",
      "includ: 46\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "privat\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "bathroom: 45\n",
      "ensuit: 38\n",
      "room: 31\n",
      "you: 23\n",
      "washroom: 17\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "util\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "includ: 275\n",
      "all: 177\n",
      "internet: 122\n",
      "and: 91\n",
      "furnish: 59\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "bu\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "stop: 248\n",
      "rout: 158\n",
      "minut: 107\n",
      "min: 107\n",
      "close: 96\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "mainten\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "prime: 9\n",
      "servic: 9\n",
      "emerg: 9\n",
      "call: 9\n",
      "common: 4\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "laundri\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "site: 142\n",
      "park: 77\n",
      "includ: 66\n",
      "internet: 62\n",
      "and: 60\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "kitchen\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "and: 65\n",
      "with: 63\n",
      "room: 42\n",
      "bathroom: 40\n",
      "full: 37\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "femal\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "for: 53\n",
      "onli: 51\n",
      "all: 41\n",
      "roommat: 40\n",
      "prefer: 39\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "gender\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "prefer: 12\n",
      "each: 8\n",
      "microwav: 8\n",
      "includ: 8\n",
      "avail: 4\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "gui\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "hei: 4\n",
      "and: 3\n",
      "look: 3\n",
      "still: 2\n",
      "laurier: 2\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "wifi\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "unlimit: 13\n",
      "and: 9\n",
      "laundri: 8\n",
      "includ: 6\n",
      "free: 6\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "dog\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "the: 3\n",
      "have: 2\n",
      "happi: 2\n",
      "hau: 2\n",
      "even: 1\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "cat\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "have: 4\n",
      "friendli: 4\n",
      "and: 3\n",
      "live: 3\n",
      "that: 2\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "boi\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "girl: 3\n",
      "bathroom: 2\n",
      "and: 1\n",
      "ramdev: 1\n",
      "from: 1\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "male\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "all: 9\n",
      "femal: 9\n",
      "and: 6\n",
      "unit: 5\n",
      "bathroom: 5\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "furnish\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "fulli: 399\n",
      "room: 128\n",
      "bedroom: 87\n",
      "with: 86\n",
      "all: 76\n"
     ]
    }
   ],
   "source": [
    "#verify the word is being used in the desired context most of the time\n",
    "from collections import Counter\n",
    "a = Counter()\n",
    "\n",
    "for keyword in keyword_neighbours:\n",
    "    print ''\n",
    "    print '++++++++++++++++++++++++++++++++++++++++++'\n",
    "    print keyword\n",
    "    wordCount = {}\n",
    "    print '++++++++++++++++++++++++++++++++++++++++++'\n",
    "    for neighbours in keyword_neighbours[keyword]:\n",
    "        for word in neighbours:\n",
    "            if word not in wordCount:\n",
    "                wordCount[word] = 1\n",
    "            else:\n",
    "                wordCount[word] = wordCount[word] + 1\n",
    "    for k, v in Counter(wordCount).most_common(5):\n",
    "        print '%s: %i' % (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyword_count = {}\n",
    "for keyword  in keywords:\n",
    "    keyword_count[keyword] = 0\n",
    "\n",
    "for user in content_tokenized:\n",
    "    all_content = []\n",
    "    for content in content_tokenized[user]:\n",
    "        all_content.extend(content)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in all_content:\n",
    "            keyword_count[keyword] = keyword_count[keyword] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furnish: 392\n",
      "internet: 296\n",
      "bu: 295\n",
      "util: 265\n",
      "laundri: 262\n",
      "park: 218\n",
      "bathroom: 185\n",
      "kitchen: 149\n",
      "femal: 122\n",
      "condit: 53\n",
      "gym: 44\n",
      "privat: 36\n",
      "pet: 28\n",
      "wifi: 28\n",
      "girl: 27\n",
      "male: 24\n",
      "facil: 20\n",
      "gui: 18\n",
      "cat: 10\n",
      "gender: 9\n"
     ]
    }
   ],
   "source": [
    "for k, v in Counter(keyword_count).most_common(20):\n",
    "    print '%s: %i' % (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n"
     ]
    }
   ],
   "source": [
    "filter_count = 0\n",
    "\n",
    "for user in content_tokenized:\n",
    "    all_content = []\n",
    "    for content in content_tokenized[user]:\n",
    "        all_content.extend(content)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        if keyword in all_content:\n",
    "            filter_count = filter_count + 1\n",
    "            break\n",
    "            \n",
    "print filter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_content = []\n",
    "for user in content_tokenized:\n",
    "    for content in content_tokenized[user]:\n",
    "        all_content.extend(content)\n",
    "\n",
    "all_count = Counter(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sublet: 762\n",
      "walk: 748\n",
      "furnish: 670\n",
      "internet: 485\n",
      "minut: 430\n",
      "fulli: 429\n",
      "laundri: 389\n",
      "util: 384\n",
      "spring: 377\n",
      "messag: 376\n",
      "bathroom: 350\n",
      "min: 337\n",
      "park: 332\n",
      "close: 327\n",
      "free: 297\n",
      "hous: 290\n",
      "kitchen: 276\n",
      "locat: 271\n",
      "leas: 262\n",
      "street: 258\n",
      "stop: 255\n",
      "high: 246\n",
      "negoti: 235\n",
      "inclus: 235\n",
      "larg: 230\n",
      "ensuit: 228\n",
      "summer: 221\n"
     ]
    }
   ],
   "source": [
    "for k, v in all_count.most_common(50):\n",
    "        if k not in myStop:\n",
    "            if k not in ['room', 'includ', 'waterloo', 'avail', 'bedroom', 'look', 'bu', 'ontario', 'pleas'\n",
    "                        , 'month', 'august', 'mai', 'price', 'unit', 'wlu']:\n",
    "                print '%s: %i' % (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    }
   ],
   "source": [
    "filter_count = 0\n",
    "\n",
    "for user in content_tokenized:\n",
    "    all_content = []\n",
    "    for content in content_tokenized[user]:\n",
    "        all_content.extend(content)\n",
    "    \n",
    "    for keyword in main:\n",
    "        if keyword in all_content:\n",
    "            filter_count = filter_count + 1\n",
    "            break\n",
    "            \n",
    "print filter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyword_count = {}\n",
    "for keyword  in main:\n",
    "    keyword_count[keyword] = 0\n",
    "\n",
    "for user in content_tokenized:\n",
    "    all_content = []\n",
    "    for content in content_tokenized[user]:\n",
    "        all_content.extend(content)\n",
    "    \n",
    "    for keyword in main:\n",
    "        if keyword in all_content:\n",
    "            keyword_count[keyword] = keyword_count[keyword] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'femal': 122, 'gender': 9, 'intern': 1, 'male': 24, 'park': 218, 'pet': 28}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
